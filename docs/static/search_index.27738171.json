[{"id":1,"title":"Seagate TurboBoost and Advanced Write Caching","content":"#\n\nTraditional HDDs predictably respond to higher workloads with slower response\ntimes. At 200 IOPS, a drive array might provide a 5ms response, but at 600 IOPS,\nlatency could rocket to 40ms or higher.\n\n传统的 HDD 以较慢的响应时间来应对较高的工作负载\n\n * 在 200 IOPS 时，硬盘阵列可能提供 5 毫秒的响应\n * 在 600 IOPS 时，延迟可能飙升至 40 毫秒甚至更高。\n\nTurboBoost 是在传统硬盘中加入 NAND 闪存，集成了\n\n * 传统的多段式缓存\n * 少量的 eMLC NAND，其特点是比消费级 MLC 的耐用性要强得多 适量的 NAND 能够满足价值需求，以足够低的成本提供最大的性能优势\n\n\n\nNVC：non-volatile cache Back electromotive force（back\nEMF）指与电机线圈在磁场中运动产生的电流方向相反的电压 HDD 可以在断电后立即利用反电动势，将 DRAM 中的数据写入 NVC Flash 中。有了 NVC\n保护的写缓存，在享受到性能好处的同时，仍然可以与关闭 write cache 同等的保护。\n\n\n\n内置的 NOR Flash 大小：2M","routePath":"/notes/hw/seagate-turboBoost","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":2,"title":"","content":"","routePath":"/notes/","lang":"","toc":[],"domain":"","frontmatter":{"overview":true},"version":""},{"id":3,"title":"dm-delay","content":"#\n\n\n简单使用#\n\n\n\n\ndelayed 参数#\n\n\n\n\n暂停 I/O#\n\n\n\n\n检查 delayed 设备的 I/O 延迟#\n\n","routePath":"/notes/linux/dm-delay","lang":"","toc":[{"text":"简单使用","id":"简单使用","depth":2,"charIndex":3},{"text":"delayed 参数","id":"delayed-参数","depth":2,"charIndex":13},{"text":"暂停 I/O","id":"暂停-io","depth":2,"charIndex":29},{"text":"检查 delayed 设备的 I/O 延迟","id":"检查-delayed-设备的-io-延迟","depth":2,"charIndex":41}],"domain":"","frontmatter":{},"version":""},{"id":4,"title":"What is /dev/fd0","content":"#\n\nVMware guests usually have floppy device in its default hardware configuration.\nAt RHEL installation, the initramfs is built with floppy kernel module because\nthe hardware is present within the configuration. Because of that, /dev/fd0 is\nalways created at boot time within the guest. But the /dev/fd0 is seldom\npopulated with any media image from the VMware hypervisor, so any attempts to\nread the device result in the I/O error being reported.\n\nThe sosreport, parted, and other commands such as third party monitoring tools,\nwhen run, interact with storage devices. This interaction typically includes\nopening the device and attempting io to the device. For sosreport and parted,\nthe io is typically to sector 0 to determine if a partition table exists. With\nno media present within the virtual floppy drive, the io fails and the messages\nabove results.\n\nReading from a floppy device when no media is present is expected to return this\nerror, its normal and as such can be safely ignored. However, to prevent these\nevents from appearing in logs the floppy device is often just removed from the\nVMware configuration for the guest as the floppy is not often (ever?) used\nwithin most guests.","routePath":"/notes/linux/fd0","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":5,"title":"ESXi 7.0 如何删除 VMFSL","content":"#\n\n最佳方式是在安装ESXi的时候，引导后，按Shift+O键。\n\n敲以下命令：\n\n\n\n注意大小写，回车安装即可。\n\n","routePath":"/notes/misc/esxi-vmfsl","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":6,"title":"inode 占用","content":"#\n\n","routePath":"/notes/misc/inode","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":7,"title":"JVM thread stack size","content":"#\n\n","routePath":"/notes/misc/jvm-thread-stack-size","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":8,"title":"Intel Ultra Path Interconnect (UPI) Bandwidth","content":"#\n\nIntel Ultra Path Interconnect (UPI) Bandwidth\n\n> A google search for \"intel UPI bandwidth\" found Skylake Processors which\n> connects GT/s and GB/s: 2 links between a pair of sockets means 41.6 GB/s\n> aggregate bidirectional bandwidth, or 20.8 GB/s each direction, 10.4 per link.\n> Seems pretty clear if that source is accurate.\n\nExample from the Intel Xeon Platinum 8160 (2 UPI links between chips):\n\n * Local Bandwidth for Reads (each socket) ~112 GB/s\n * Remote Bandwidth for Reads (one-direction at a time) ~34 GB/s\n * Local bandwidth scales perfectly in two-socket systems, and remote bandwidth\n   also scales very well when using both sockets (each socket reading data from\n   the other socket).","routePath":"/notes/misc/ultra-path-interconnect","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":9,"title":"结论","content":"Optimizing Caching on Modern Storage Devices with Orthus#\n\n高质量存储论文（一）\n\n\n概述#\n\n本文介绍了非层次缓存（NHC，non-hierarchical caching），现代存储层次结构中的一种新式缓存方法。与传统缓存相比，NHC\n通过将过载重定向到层次结构中较低的设备（如果这么做有好处）来提高性能。NHC 动态调整分配和访问策略，以最大限度提高性能（如高吞吐，较低的 99%\n延迟）。我们在 Orthus-CAS（块层 cache 内核模块，基于 OpenCAS）和 Orthus-KV（用户态 kv 缓存层，基于\nWisckey）中实现了 NHC。我们通过深入的实证研究展示了 NHC 的性能：在一系列实际的工作负载下，Orthus-CAS 和 Orthus-KV\n在各种现代层次结构上提供显著的更好性能（高达两倍）。\n\n\n场景#\n\n这种高命中率和高负载在生产缓存系统中很常见。 例如，Twitter 最近的一项研究表明，十个 Twemcache 集群中有八个的未命中率低于 5% [98]。\n研究还表明，缓存层经常承受重负载（即，它们是带宽饱和的）[17, 56]。\n\n\n简介#\n\n存储层次结构的概念长期以来一直是计算机系统设计的中心。事实上，在广泛使用的教科书上可以找到关于层次结构及其基本性质的假设：快速存储器是昂贵的，所以存储器层次结构\n被组织为几个级别，每个比下个级别更小、更快、更贵的字节都离处理器更远。\n\n为了处理层次结构的性质，系统通常会采取两种策略：\n\n * 缓存（cache）\n * 分层（tiering）\n\n考虑具有两个存储层的系统：性能层（快，小，贵）和容量层（慢，大，便宜） 使用缓存时，所有的数据都驻留在容量层。热数据的副本通过缓存替换算法放置于性能层中。\n采用分层时，热数据放置于性能层中，并且会在一段时间内迁移数据（而不是复制数据）。如果足够多的请求被发送到性能层，则总体性能接近于性能层的峰值性能。因此，传统的缓\n存和分层策略都努力确保大多数访问命中性能层。\n\n虽然这种优化命中率的传统艺能可能仍然使用于传统的层次结构（例如 CPU cache vs DRAM，或是 DRAM vs\n硬盘），但存储设备的快速演进在现代存储层次结构中使这种说法变得复杂。实际上，很多新的 NVM 和低延迟 SSD\n的出现引入了具有重叠性能特征的设备（也就是层次之间的性能差距很小，并且可能在某些情况下有重叠）。因此，必须重新考虑如何在存储分层结构中管理此类设备。\n\n为了更好地理解这个问题，考虑一个两级的层次结构：\n\n * 容量层：基于 Flash 的固态硬盘\n * 性能层：看起来更快的 Optane\n\n在某些情况下，Optane 优于\nFlash，因此传统的缓存/分层结构运行良好。然而，在其他情况下，比如高并发的工作负载时，设备的性能相似（也就是此时的存储层次结构不是一个层次结构），因此传统的\n缓存和分层没有利用容量层可用的全部带宽。我们需要一种不同的方法来最大化性能。\n\n为了解决这个问题，我们引入了非层次缓存\nNHC。这是一种用于现代存储层次结构的新缓存方法。核心思想：当传统缓存向性能层发送的请求多于有用的请求时，一些额外的负载可以动态地转移到容量层设备。\n这在两个方面对传统缓存进行了改进：\n\n 1. 通过监控性能并调整发送到每个设备的请求，NHC 可以从容量层设备中提供更多可用的性能\n 2. 当设备间的数据移动不能提高性能时，NHC 会避免这样的移动\n\n虽然将过量负载重定向到层次结构中较低的想法同时适用于缓存和分层，但我们的重点放在缓存上。\n\n先前的工作已经解决了缓存的一些限制，将多余写入从 SSD offload 到 HDD。然而有两个关键的局限性：\n\n * 它们不对缓存中存在的项的访问链路进行重定向\n * 它们不适应不断变化的工作负载和并发级别（对于现代设备来说尤为重要）\n\n我们在两个系统中实现了 NHC：Orthus-CAS 和 Orthus-KV。在负载较低的情况下，Orthus\n的行为类似于传统缓存。在其他情况下，他们将缓存层的过重负载卸载到容量层，从而提高了性能。通过严格的评估，在各种实际设备和模拟设备上，Orthus\n的实施极大地提高了性能（高达两倍）。\n\n\n动机#\n\n\n管理存储分层结构#\n\n存储层次结构由下面两部分组成：\n\n * 多个异构存储设备\n * 在这些设备之间传输数据的策略\n\n为了简单起见，我们假设一个由一个或两个设备组成的层次结构：\n\n * 性能设备 $D_$\n * 容量设备 $D_$\n\n传统上有两种方法用于管理这种层次结构：缓存和分层。\n\n\n\n使用缓存时，热数据从 $D_$ 复制到 $D_$（在每次未命中时）。为了给这些热数据腾出空间，缓存通过 ARC、LRU 或者 LFU\n等算法驱逐冷数据。数据移动的粒度通常在 4k block。\n\n分层类似于缓存，在性能设备中维护热数据。然而，与缓存不同的是，访问 $D_$ 上的数据时，不一定被提升为 $D_$。数据可以直接从 $D_$\n中提供。数据仅在较长的时间范围（数小时/数天）内定期迁移，并执行长期优化的数据排布。分层通常以较粗的粒度（如整个卷/较大的\nextent）。虽然缓存可以对工作负载的变化做出快速的反应，但是分层无法做到。\n\n为了最大限度提高性能，缓存和分层都努力确保从高性能设备为大多数的访问提供服务。因此，大部分缓存和分层都旨在最大限度的提高性能设备的命中率。\n\n\n存储硬件演进趋势#\n\n\n\n * 延迟在降低\n * 带宽差距不明确\n\n为了更好地了解这些设备的性能重叠，图 2 显示了在改变并发级别的情况下，针对 4KB 读/加载和写/存储的各种实际设备的吞吐量。\n\n\n\n读：\n\n * 低并发读：设备对之间有显著差异\n * 高并发读：Optane 和 Flash 的性能几乎相同\n\n写：\n\n * 低并发写：/\n * 高并发写：由于 NVM 并发写入的低性能，在 NVM/Optane 中，比率从低并发的好得多变成高并发时的差得多\n\n总结：\n\n * 与传统的层次结构不同，新的存储层次结构可能不是层次结构，相邻层的（NVM/Optane）的性能可能相似\n * 新设备的性能取决于不同的工作负载和并发级别。\n\n\n传统和现代存储层次结构中的缓存#\n\n 1. 对缓存性能建模\n 2. 实证分析，填充模型没有的重要细节\n 3. 对 Splitting 的方法进行了建模，突出传统缓存的缺点\n    1. Splitting 时，数据只是跨设备拆分，运行时不执行任何迁移。\n    2. 当接入链路在性能设备和容量设备之间得到最佳的 splitting 时，splitting 的性能优于缓存\n\n不妨假设\n\n * 有两个设备 $D_$ 和 $D_$，分别以 $R_$ 和 $R_$ op/s 执行\n * 工作负载具有很低并发性（一次一个请求）或者高并发性（一次很多请求）\n * 只有读（简化分析，不需要考虑 cache replacement 中的 dirty writeback）\n\n\n建模#\n\n缓存命中率 hit rate：$H \\in [0, 1]$\n\n考虑两个极端：极低的并发和极高的并发\n\n\n每次一个请求#\n\n每次一个请求，每个请求的平均时间为：\n\n$$ T_=H \\cdot T_ + (1-H) \\cdot T_ $$\n\n$T_$ 是 fast device 速率的倒数\n\n$$ T_=\\frac R_ $$\n\n$T_$ 是从 slow device 获取数据并且写入到 fast device 的成本\n\n$$ T_=\\frac+\\frac $$\n\n对应的带宽：\n\n$$ B_=\\frac=\\frac $$\n\n\n每次 N 个请求#\n\n命中的请求数：$H \\cdot N$\n\n未命中请求数：$(1-H)\\cdot N$\n\n命中的请求直接由 fast device 返回。未命中的请求先访问 fast device 再访问 slow device，所以不管是命中还是不命中 fast\ndevice，都需要访问 fast device。\n\n$$ T_(N)=N \\cdot(1-H) \\cdot \\frac \\ T_(N)=N \\cdot(1-H) \\cdot \\frac+N \\cdot H\n\\cdot \\frac=N \\cdot \\frac $$\n\n总完成时间取决于最后完成的设备：\n\n$$ \\begin T_(N) &=\\max \\left(T_(N), T_(N)\\right) \\ &=\\max \\left(N \\cdot \\frac, N\n\\cdot \\frac\\right) \\end $$\n\n带宽为：\n\n$$ B_=\\frac $$\n\n\n用于对照的 Splitting 策略#\n\nSplit rate：$S\\in[0,1]$，表示 $S$ 份额的请求数由 $D_$ 服务，$(1-S)$ 份额的请求由 $D_$ 服务。\n\n带宽为：\n\n$$ \\begin B_=\\frac \\ B_=\\frac \\end $$\n\n\n解析模型#\n\n\n\n$$ Ratio=\\frac $$\n\n纵轴表示带宽，横轴表示 Cache 命中率或 Split 率。令 $R_=100$。\n\n第一张图。$Ratio=100:1$，显示的是传统架构下的 Cache 策略和 Split 策略的比较。随着命中率的提升，系统的带宽也随之提升。即便是命中率为\n$80%$，总性能也相当低，因为慢速设备带来的惩罚较高（$R_=1$）。\n\n图二。$Ratio=100:10$，此时 $R_=100, R_=10$。需要注意的细微差别：高并发下，即使命中率略低于\n$1$，也能实现完美的性能，因为未完成的请求隐藏了未命中的惩罚。\n\n图三。$Ratio=100:50=2:1$，用以模拟现代的存储层次结构。两点发现。\n\n 1. Cache 受限于 $D_$ 的性能，无法实现两种设备的综合性能。\n 2. 最大化 $D_$ 响应的请求数并不总是能提供最佳性能。在大量并发的情况下，当大约三分之二的请求指向 $D_$ 时，Split 实现了 $D_$ 和\n    $D_$ 的聚合带宽。进一步增加分流率只会降低性能。 因此，在现代层次结构中，关键不是最大化命中率或拆分率，而是找到必须发送到每个设备的正确比例的请求。\n\n图四。对照。\n\n\n真实世界#\n\n传统层次结构\n\n * DRAM + Flash SSD\n\n现代层次结构：\n\n * NVM + Optane，Optane DCPM 128GB+Optane 905P SSD\n * Optane SSD + Flash SSD\n\nBench 工具 HFIO（Hierarchical Flexible I/O Benchmark Suite）：\n\n * LRU-replacement policy for caching\n * HFIO 生成具有各种参数的复合工作负载（例如，混合读写、局部性、并发数）。\n * HFIO 精确控制 Cache Layer 大小和访问位置以获得所需的命中率\n * bs=32k\n * 只做随机读写\n\n\n\n上图中，第二行和第三行低并发时，$D_$ 没有被充分利用，因此提高命中率/拆分率可以提高性能。\n\n高并发时，最大化命中率不会得到峰值性能。这些情况下，容量设备提供了可观的性能，因此在最佳拆分率的情况下，Split 提供的性能比 Cache 更好。\n\n\n\n实验揭示了模型没有的复杂性：最佳拆分率取决于几个因素：\n\n 1. 并行度\n\n 2. 读写比例\n    \n    Figure 5 中可以看到，对于 Optane+Flash，读密集型负载的最佳拆分率为 $90%$，写密集型负载的最佳拆分率是 $60%$，这是因为\n    Optane 和 Flash 的写入性能差异小于读取性能差异。NVM+Optane 也存在类似结果。\n\n\n总结和启示#\n\n * 经典缓存在现代层次结构中不再有效：它没有利用容量层可以提供的相当大的性能。\n\n * 在高命中率和高速缓存层负载过重的情况下，一些请求可以卸载到容量设备。\n   \n   这种高命中率和高负载在生产缓存系统中很常见。 例如，Twitter 最近的一项研究表明，十个 Twemcache 集群中有八个的未命中率低于 5%\n   [98]。 研究还表明，缓存层经常承受重负载（即，它们是带宽饱和的）[17, 56]。\n\n解决方案：\n\n * 增加层次结构中缓存设备的数量； 然而，这种方法可能非常昂贵，因为性能设备的成本更高。\n * 将请求卸载到容量层提供了一种更经济的方式来实现显着的改进。 这种卸载方法可以通过优化将请求拆分到每个设备来提供所有设备的总体性能。 为了使 Split\n   方法运行良好，动态调整拆分速率至关重要，因为在现代层次结构中，最佳速率变化很大，具体取决于写入比率和并发级别等因素。\n\nTiering 存在与现代层次结构中的缓存类似的缺点。\n\n在这项工作中，我们专注于改进缓存有两个主要原因\n\n首先，从根本上讲，将访问分层以优化分割是很困难的：\n\n * 迁移或复制以匹配当前分层中的最佳拆分可能会损害性能。 相比之下，缓存可以轻松绕过缓存命中容量设备； 热数据的副本在两台设备上始终可用。\n\n * 其次，我们认为在许多情况下，缓存可能是唯一合适的解决方案，而分层可能不合适。 例如，应用程序只能在需要持久性时使用 DRAM 作为缓存，并且不能在\n   DRAM+NVM 层次结构中分层。\n\n\nNHC：Non-Hierarchical Caching#\n\n\n设计目标#\n\n 1. 性能比经典缓存结构更好。尝试找到最佳的命中率。最坏的情况下退化到经典缓存。\n 2. 零额外配置，没有比经典缓存更多的假设\n 3. 适应动态变化的负载\n\n\n主要思想#\n\n为了提高整体性能，将多余负载卸载到容量设备上。\n\n如何实现？预热时，识别当前工作集，数据加载到 $D_$ 中。命中率稳定后，多余负载发送到 $D_$\n\n多余的负载由两个部分组成：\n\n 1. 读命中缓存时，$D_$ 已经满载。\n 2. 读未命中缓存时，传统缓存结构从 $D_$ 移动数据到 $D_$ 以提高缓存命中率。然而提高缓存命中率并不能提高性能，因为 $D_$ 已经满载。\n\nNHC 通过基于反馈的方法确定超载。如果工作负载永远在变化，降级为传统缓存。\n\n对设备特性做出假设：\n\n 1. 设备性能存在上限\n 2. 增加负载不会降低性能\n 3. 设备性能达到上限前，随着负载的提高，$D_$ 的性能增幅比 $D_$ 高。\n\n经典缓存尝试找到一个工作集使得命中率最高。\n\n\n架构#\n\n\n\n相比传统缓存，增加了：Cache controller 和 Cache Scheduler。\n\n两个变量：\n\n * data_admit（$da$）控制 read miss 的行为\n   \n   $da=1$，在 $D_$ 中通过缓存替换策略分配\n   \n   $da=0$，read miss 由 $D_$ 处理。经典缓存就是 $da=1$ 的情况。\n\n * load_admit（$la$）控制 read hit 的行为\n   \n   每次 read hit 时生成一个随机数 $R\\in[0,1.0]$，如果 $R\\leq la$，请求给 $D_$，否则给 $D_$。传统缓存就是\n   $la=1$ 的情况，每次请求都先给 $D_$ 处理。la=0 时，所有命中的读发送到 $D_$。\n\nNHC 对于写入：\n\n * $da$ 和 $la$ 不控制 write hit/misses\n * Writeback 可能会在 $D_$ 上引入脏数据，$D_$ 上有旧数据。此时 NHC 不会向 $D_$ 发送读。\n\n\nCache Scheduler Algorithm#\n\n调度程序有两种状态：\n\n 1. 增加在 $D_$ 上缓存的数据量，最大化命中率\n 2. 保持缓存中的数据不变，调整发送到每个设备的负载。\n\n\n\n$f(x)$ 是一个函数，返回 $la=x$ 时的性能。如何得到对应的性能？通过设置 $la=x$ 一段时间测量得到性能。\n\nstep 是 load_admit 迭代的 step size\n\n大循环，每次循环做这些事情：\n\n提高 hit rate\n\n 1. $da=1$，$la=1$（此时行为类似于传统缓存，每次请求都先给 $D_$ 处理）\n 2. 等待缓存命中率稳定（如何判定 stable？）\n 3. $da=0$（请求都通过 $D_$ 处理）\n\n调整 $la$\n\n 1. $ = 当前 $\n 2. 迭代\n    * $ratio=1$\n    * $max_f$ = 测一下 $la=ratio-step$ 的性能，测一下 $la=ratio$ 的性能，测一下 $la=ratio+step$\n      的性能，在这三个性能中取最大值\n    * 如果 $ratio-step$ 性能最好，设置 $la=ratio-step$\n    * 如果 $ratio+step$ 性能最好，设置 $la=ratio+step$\n    * 如果 $ratio$ 性能最好，设置 $la=ratio$。如果 la==1，则从 A 开始重新迭代\n    * 如果当前的 $hit\\ rate$ 比一开始记录的 $start_hit_rate$ 更低，就从 A 开始重开\n\n\n实现#\n\n什么是 hit rate stable？最后 100ms 中，命中率的变化在 0.1% 以内。简单的启发式算法，但很有效。原因是：轻量的工作负载下，NHC\n可以快速切换到传统缓存。高负载下，更高的 hit rate 会让 NHC bypass more hits\n\n\n评估#\n\nWith 95% hit ratio and Load-2.0, NHC obtains improvements of 21%, 32%, 54% for\nDRAM+NVM, NVM+Optane, and Optane+Flash, respectively. Such im- provements are\nmarginally reduced with an 80% hit ratio.\n\n第三，在这些层次结构中，Optane+Flash 与 Orthus-CAS 的改进最大，因为 Optane 和 Flash 之间的性能差异最小，其次是\nNVM+Optane 和 DRAM+NVM。 我们使用 FlashSim 的结果显示了从业者如何预测在其目标层次结构上使用 NHC 的改进。\n\n最后，我们的测量表明 Orthus-CAS 适应复杂的设备特性。 在 80% 的命中率下，经典缓存在任何实际层次结构上都无法达到 1.0\n的标准化吞吐量，因为缓存未命中会向缓存设备引入额外的写入。 NHC 处理这种复杂性。\n\n\n\n\n\n\n\n\n结论#\n\n在本文中，我们展示了新兴存储设备如何对现代层次结构中的缓存产生重大影响。 我们引入了非分层缓存，这是一种优化的新方法，可从现代设备中提取峰值性能。 NHC\n基于一种新颖的缓存调度算法，该算法考虑工作负载和设备特性来做出分配和访问决策。 通过实验，我们展示了 NHC 在各种设备、缓存配置和工作负载上的优势。 我们相信\nNHC 可以作为管理存储层次结构的更好基础。","routePath":"/notes/paper/orthus","lang":"","toc":[{"text":"管理存储分层结构","id":"管理存储分层结构","depth":2,"charIndex":1802},{"text":"存储硬件演进趋势","id":"存储硬件演进趋势","depth":2,"charIndex":2292},{"text":"建模","id":"建模","depth":2,"charIndex":2946},{"text":"每次一个请求","id":"每次一个请求","depth":3,"charIndex":3003},{"text":"每次 N 个请求","id":"每次-n-个请求","depth":3,"charIndex":3215},{"text":"用于对照的 Splitting 策略","id":"用于对照的-splitting-策略","depth":3,"charIndex":3633},{"text":"解析模型","id":"解析模型","depth":2,"charIndex":3771},{"text":"真实世界","id":"真实世界","depth":2,"charIndex":4306},{"text":"总结和启示","id":"总结和启示","depth":2,"charIndex":4948},{"text":"NHC：Non-Hierarchical  Caching","id":"nhcnon-hierarchical--caching","depth":2,"charIndex":-1},{"text":"设计目标","id":"设计目标","depth":3,"charIndex":5645},{"text":"主要思想","id":"主要思想","depth":3,"charIndex":5733},{"text":"架构","id":"架构","depth":3,"charIndex":6077},{"text":"Cache Scheduler Algorithm","id":"cache-scheduler-algorithm","depth":3,"charIndex":6543},{"text":"实现","id":"实现","depth":3,"charIndex":7229}],"domain":"","frontmatter":{},"version":""},{"id":10,"title":"FAST23 Pangu","content":"More Than Capacity: Performance-oriented Evolution of Pangu in Alibaba#\n\nMore Than Capacity: Performance-oriented Evolution of Pangu in Alibaba\n\n\n摘要#\n\n本文介绍了盘古存储系统如何随着硬件技术的发展和商业模式的变化进行演进，以提供高性能、高可用的存储服务。IO 延迟达到 100us 的水平。 发展分为两阶段：\n\n 1. 迎接 SSD 和 RDMA，大幅降低 io 延迟，提供高吞吐高 iops\n 2. 容量导向转变为性能导向\n    1. 使用更大的 ssd\n    2. RDMA 25GbE 升级为 100GbE\n    3. 引入了一系列关键设计：\n       1. 减少流量放大\n       2. 远程缓存直接访问\n       3. CPU 计算卸载\n\n\n简介#\n\n * 承载阿里核心业务（电商、支付）\n * 万亿文件、EB 级别容量\n\n\n盘古 1.0：面向容量#\n\n2009-2015 HDD，延迟毫秒级 网络，Gbps 级别 盘古 1.0 基于 ext4 和内核态 TCP 性能达到了 HDD 和 Gbps 网络的极限\n用户此时关注于获得大量空间，而不是高性能\n\n\n盘古 2.0#\n\n2015 开始，开发盘古 2.0 目标：提供高性能存储服务 延迟 100us 级别 观察到：\n\n 1. 盘古 1.0 中使用的多种文件类型，特别是允许随机访问的文件类型在 ssd 上表现不佳。ssd 可以在顺序操作上实现高吞吐和 iops\n 2. 由于数据复制和频繁中断，内核空间软件栈无法跟上 ssd 和 rdma 的高 iops 和低延迟\n 3. the paradigm shift from [[学习/server-centric datacenter architectures]] to\n    [[学习/resource-disaggregated datacenter]] architectures poses additional\n    challenges to achieving low I/O latency\n    1. 数据中心架构发生转变：以服务器为中心 -> 资源分离的数据中心架构\n\n\n第一阶段#\n\n * 设计了一个统一的，append-only 的持久化层\n * 引入自包含分块布局 (a self-contained chunk layout)\n   * 以降低文件写入的 io 延迟\n * 设计了一个用户空间的存储操作系统 USSOS\n   * run-to-completion 的线程模型，实现用户空间存储栈和用户空间网络栈之间的高效协作\n   * 用户空间的调度机制，实现足够的 CPU 和内存资源分配\n   * 动态 SLA 保证机制 实现 P 999 毫秒级 io 延迟\n\n\n第二阶段#\n\n适应以性能为导向的商业模式，进行基础设施更新，突破网络/内存/CPU 瓶颈 2018 以来，越来越多的企业将业务转移到阿里云，他们对存储性能和延迟有严格要求。\n新冠以来转变更快。 沿着基于 clos 的拓扑（如 [[学习/FatTree]]\n结构）用原有的服务器和交换机扩展不够经济：高总成本（空间、电力、冷却、劳动力），高环境成本（碳排放）。因此盘古开发了内部的高容量存储服务器（每台 96TB\nSSD），25GbE 升级到 100GbE。 提出一系列技术应对性能瓶颈，充分利用资源：\n\n * 降低网络流量放大率，动态调整流量优先级\n * RDCA：远程直接缓存访问应对[[学习/内存瓶颈]]\n   * 消除序列化反序列化\n   * 引入 CPU wait 指令，在超线程间同步解决 CPU 瓶颈\n\n\n小结#\n\n一阶段结束时，盘古 2.0 成功支持弹性 SSD 块存储服务，延迟在 100us 级别，iops 一百万。2018 双十一，盘古 2.0 支持阿里数据库延迟为\n280us。对于 OTS 存储，他在盘古 2.0 中的延迟比在 1.0 中低一个数据级。写密集服务（如 EBS 云盘），P 999 的延迟小于\n1ms。对于读密集型服务（在线搜索），P 999 低于 11ms。 第二阶段，网络从 2 x 25 Gbps 升级到 2 x 100 Gbps，突破了网络内存和\nCPU 的瓶颈，每台 taishan 存储服务器（normalize）吞吐增加 6.1 倍。\n\n\n背景#\n\n\n盘古的概述#\n\n\n\n由 Pangu Core、Pangu Service、Pangu Monitoring System 组成，通过高速网络相互连接。\n\nPangu Core：提供了 append-only 持久化语义。\n\n * client：heavyweight client，为 service 提供 SDK，接收文件请求，与 master 和 chunkserver\n   通信响应请求。实现副本管理、SLA 保证、数据一致性。\n * master：元数据服务，基于 Raft。\n   * 分解为两个服务，namespace 服务和 stream meta 服务。stream\n     是一组块的抽象。这两个服务通过目录树划分元数据（实现元数据局部化）、使用 hashing 实现负载均衡。namespace\n     服务提供文件信息（目录树、namespace（应该是多租户的概念？）），stream meta 提供文件到 chunks 的映射（比如 chunk\n     的位置）。\n * chunkserver：以 chunk 形式存储数据，配备一个用户空间存储系统 USSFS\n   * USSFS 为不同硬件提供高性能、append-only 的存储引擎\n     * 比如用于叠瓦盘的 SMRstore\n     * 盘古 2.0 一阶段中，每个文件三副本，后来引入垃圾收集 GCworker，EC 存储文件\n     * 盘古 2.0 二阶段中，关键业务使用 EC 取代三副本，降低流量放大。\n\n基于 Pangu Core，提供传统的云存储服务（EBS、OSS、NAS），以及面向云的文件系统 Fisc。\n\nPangu Monitoring System（监控系统）\n\n * 比如 Perseus 为 Pango core 和 Pangu service 提供实时监控，和 AI 辅助的 root cause 分析服务\n\n\n设计目标#\n\n * 低延迟\n   * 平均 100us\n   * 即使有网络抖动和服务器故障也提供 ms 级别 P999 SLA\n * 高吞吐：吃满硬件\n * 统一的高性能底座\n\n\n阶段一#\n\n\nAppend-only fs#\n\n盘古的持久层提供了与盘古所有存储服务（如 EBS、OSS 和 NAS）的接口。 在盘古的早期开发中，持久层为不同的存储服务提供不同的接口。 例如：它为低延迟的\nNAS 服务提供 LogFile 接口，为高吞吐量的 Maxcompute 数据分析服务提供 TempFile 接口。\n然而，这种设计带来了大量的开发和管理的复杂性。具体来说，对于每一个存储服务，盘古的开发人员都必须设计和实现一个新的接口。这是一个复杂的、劳动密集型的、容易出错的\n过程。 为了简化开发和管理，引入了统一的文件类型：FlatLogFile 盘古的开发者必须确保通过 FlatLogFile\n进行的数据操作，尤其是写操作，能够在存储介质上高效、可靠地执行。因此，所有的的升级和改变对盘古的开发者来说是透明的。储存服务的升级和改变对盘古开发人员来说是透明\n的，这大大简化了盘古的开发丰富了盘古的开发和管理。 为了确保通过 FlatLogFile 进行的数据操作能够在 SSD 上有效执行，我们对\nFlatLogFile 的顺序操作进行了对齐以实现高性能。\n\n\nHeavyweight Client#\n\n从 master 获得 chunk 信息，client 负责进行复制或者 EC。Client\n实现重试机制，应对偶尔出现的抖动（比如网络丢包）。还实现了探测机制，定期从主控获得最新的 chunkserver 状态，评估 chunkserver\n的服务质量。\n\n\nAppend-only chunk 管理#\n\n典型的文件系统（比如 ext4）将文件存储在块中，一个文件和他的元数据是通过两次写入分别写入存储介质的，增加了写入延迟，缩短了 SSD 寿命。\n为了解决这个问题，pangu 在 chunkserver 上以 chunks 的形式存储文件。每个 chunk 存储数据和自己的元数据。因此，一个 chunk\n通过一次操作（而不是两次）写入存储介质，大大降低写入延迟，提高了存储介质寿命。 一个 chunk 包括多个 sector unit。每个 sector unit\n包括：data、padding、footer。 Footer 存储 chunk 元数据：chunk id、len、CRC。 独立的分块布局允许\nchunkserver 能够从故障中自行恢复。 具体来说，当 client 将连续的独立块写入存储介质时，chunkserver 在内存中存储这些 chunk\n的元数据副本，并定期将这些存到存储介质上进行 checkpoint。 当故障发生并导致一些未完成的写入时，chunkserver 会从 checkpoint\n中加载元数据并与之进行比较。如果存在差异，chunkserver 会检查 chunks 的 CRC 以恢复到最新的正确状态。\n\n\n元数据操作优化#\n\nmaster namespace 服务负责目录树和文件管理。 stream 服务负责 chunk 信息。stream 是一组 chunk 的抽象概念。同一\nstream 中的 chunk 属于同一个文件。\n\n并行化#\n\n高内聚的元数据会被映射到不同的元数据服务器。 它还使用了一种新的数据结构：支持可预测的目录 ID，并允许客户端高效地并行执行路径解析。\n\n还有几种技术可以加快 client 从 stream 服务检索 chunk 的方式。\n\n * 长度可变的 big chunk。big chunk 的好处：降低元数据的量，避免 client 频繁请求 big chunk 造成的不必要 io\n   延迟，有助于提高 SSD 寿命。然而，简单的增加 chunk 大小会增加碎片化的风险（想想 ZBS NFS 里的小文件，一个 47B 文件占用 256M\n   逻辑空间）。因此引入了可变长 chunk（1MB~2GB）。降低了碎片化的概率，并且保持与 pangu1.0 的兼容性。\n   * EBS 服务的分块大小有 P95 为 64 MB，P99 的量化为 286.4 MB\n * Client 端缓存 chunk 信息。每个客户端有一个本地元数据缓存池，减少元数据查询请求数量。缓存池基于 LRU。当一个 service\n   想要访问数据时，client 首先查询本地的元数据缓存。当缓存未命中，或者缓存命中但是 chunkserver\n   提示元数据已过期（如由于副本迁移），client 会向 master 发出请求获得最新的元数据。\n * Chunk 信息请求的批量化（batching）。client 在很短的时间内汇总多个 chunk 信息请求，批量发送给\n   master，提高查询效率。master 并行处理成批的请求，汇总结果发回 client。\n * 贪婪的 chunk 信息预取（prefetching）。当 master 收到读取请求时，会给 client 返回相关 chunk 和其他 chunk\n   的元数据。当 master 收到写请求时，会返回更多的 chunkserver 来响应。如果遇到写入异常，可以在不请求的情况下切换\n   chunkserver。\n * 数据捎带，降低一个 RTT。client 从 master 检索到 chunk 地址后，将 chunk create\n   请求和写入数据合并为一个请求，并发送给 chunkserver。\n\n\nChunkserver USSOS#\n\n坚持通过内核空间进行数据操作的传统设计是低效的：导致频繁的系统中断消耗 CPU 资源、导致用户空间和内核空间之间不必要的数据重复（memory copy）\n为了应对这些问题，我们 bypass kernel，为 chunkserver 开发一个高性能的 userspace 存储操作系统。\n\n * 设备管理\n * run-to-completion 现成模型\n * 用户级内存管理\n * 用户空间调度\n * 为 SSD 定制的 USSFS\n\nUSSOS 基于 RDMA、DPDK、SPDK 建立，但超越并统一了网络栈和存储栈。\n在传统的管道线程模型中，一个请求被分解成各个阶段，每个阶段在一个线程上运行。相反，在 USSOS\n中，请求从头到尾都在一个线程上运行，在运行到完成的模型中，减少了由上下文切换、线程间通信和线程间同步引起的开销。\n\n\n内存管理#\n\n关于共享内存的工作有：vhost、shmipc 其次，线程请求一个巨大的内存空间，作为网络工作和存储栈之间的共享内存。具体来说，从网络收到的数据通过 RDMA\n协议存储在共享 hugepage。在发送 hugepage 的元数据（addr、size）后，可以通过 SPDK 直接从 hugepage\n写入存储介质。这样就实现了网络到存储的零拷贝。通过 io 数据的 user-level share hugepage，不同角色（比如 chunkserver 和\nGCworker）之间的数据传输也可以实现零拷贝。\n\n\nCPU 调度#\n\n防止一个任务阻塞后面的任务。每个 chunkserver 有固定数量的工作线程。分配给同一工作线程的请求以 fifo\n顺序执行。因此对于一个给定的请求，如果一个任务需要太多的时间（表查询，表搜索、遍历、内存分配、监控和统计），他将占用资源并且阻止后续任务，降低了请求的性能，使得\n其他请求处于饥饿状态。对于 heavy task，引入了心跳机制来监控任务的运行时间并设置情报。如果一个任务的时间片用完了，会被放到后台线程中，从\ncritical path 中移除。对于 system overhead，使用 tcmalloc 缓存，让高频操作在缓存中完成。 优先级调度，保证高\nQoS。不同的请求有不同的 qos 目标（用户请求：高优先级，GC：低优先级）。然而一个低优先级的请求可能会 block 后来的高优先级请求，并且由同一个\nworker thread 执行。因此很难保证一个高优先级的请求总是能优先执行。USSOS 有一个优先级队列，任务根据 qos 目标放入相应的优先级队列。\npolling 和 eventloop 的切换（NAPI）。NIC 提供一个 fd，通过 fd 事件通知应用程序的数据到达。app 默认处于事件驱动，收到\nnic 的通知切换到 polling 模式。如果一段时间内没有收到任何 io 请求，切换回事件驱动模式。\n\n\nAppend-only USSFS#\n\n通过 FlatLogFile 的 append-only 语义， USSFS 支持 append-only 写入，并提供一套基于块的语义，如 open,\nclose, seal, format, read, append 而不是像 Ext 4那 样的标准 POSIX 语义。 采用不同机制最大化 SSD 性能：\n\n * self-contained chunk：降低数据操作的次数\n * 不像 ext4 在 inode 和 file dir 之间建立层次关系，而是所有对文件的操作都记录在 log 文件中，元数据在 mount 时通过\n   replay log 来建立。\n * polling。\n\n\n高性能高可用#\n\nChasing#\n\n\n\n其中，$2 \\cdot MinCopy > MaxCopy$ 比如三副本写入两副本就可以返回成功。考虑这样的情况 [1 2 3]，[1 2] 返回成功，[3]\n在 $T$ 时没有返回成功。Client 向上层返回成功，并且在内存中保留 chunk，等待另一个毫秒级的 $t$（来自经验值）。如果 3 在 $T+t$\n能返回成功，client 释放掉内存中的 chunk。如果 3 没能完成写入，但是没有完成的部分小于另一个经验值 $k$，client 对 3\n发起重试。如果大于 $k$，client 会封存这个 chunk，以便这个 chunk 不会有后续的 append，然后 client 通知\nmaster，master 将从 [1 2] 对 3 进行数据的复制，确保最终有三副本。 在谨慎选择 $t$ 和 $k$ 的情况下，chasing\n大大减低了长尾写延迟，并且不会增加数据丢失的风险。具体来说，以 MaxCopy=3 的情况为例，在两个副本成功地完全写入 chunkservers\n后，系统中有三个副本，而第三个副本在客户端的内存中。在 $[T, T + t]$ 期间，只有当两个 chunkserver 副本的 SSD\n损坏，客户端内存中的最后一个副本也发生故障或集群断电时，才会发生数据丢失。因为 SSD 的年故障率 （ 即 1.5%∼1.5% ） 和服务器的年停机率（即 <\n2%）很接近，所以当两个副本成功写入，第三个副本被 Chasing 时，数据丢失的概率与三个副本都成功写入时的概率大致相同。\n\nNon-stop write#\n\n发生故障时，client 封存这个 chunk，向 master 报告成功写入的数据长度，然后使用一个新的 chunk\n继续写未完成的数据。如果写入密封块的数据被破坏了，我们会使用其他的副本在后台流量中把这个数据的副本复制到新的块中。如果没有副本可用，客户端会将这些数据写到新的\nchunk。\n\nBackup read#\n\nclient 收到之前发送的读取请求的响应前，向其他 chunkserver 发送额外的 read 请求作为备份。两个关键参数：发送 backup read\nrequest 的数量和等待时间。为此，pangu 计算了不同磁盘类型和 io 大小的延迟（参考 perseus），并且利用这些信息动态的调整发送 backup\nread request 的时间，并且限制了数量控制系统负载。\n\n黑名单#\n\n为了避免向服务质量差的 chunkserver 发送 io 请求，引入了两个黑名单：确定性黑名单和非确定性黑名单。当 pangu 确定一个\nchunkserver 无法使用时（例如 SSD 损坏），这个服务器添加到确定性黑名单中。如果一个 chunkserver\n可以提供服务，延迟超过了一定的阈值，添加到非确定性黑名单中，其概率随着服务延迟的增加而增加。如果该服务器的延迟超过所有服务器的延迟的中位数数倍，会被直接添加到非\n确定性黑名单中，概率为 1。为了从黑名单中释放 chunkserver，客户端定期（比如每秒）向这些服务器发送 io\n进行探测。如果确定性黑名单上的服务器成功返回请求的响应，会从黑名单中移除。如果是非确定性黑名单，根据响应的时间来进行决策。 pangu\n限制了黑名单的服务器总数，以确保系统的可用性，每个 chunkserver 引入一个宽限期。为 tcp 链路和 rdma 链路分别维护黑名单，分别进行探测。\n\n\n评估#\n\n\n\n\n阶段二：适应性能导向的商业模式#\n\n单台 taishan 服务器的 SSD 吞吐量可以达到 20GB/s 以上。有了这样高性能的存储，自然会观察到其他的资源（网络、内存、 CPU）成为 pangu\n的存储瓶颈。因此进行了网络的升级（25 Gbps->100 Gbps）。\n\n\n网络瓶颈#\n\n带宽扩展。一开始使用 lossless rdma。当 rdma 中有太多的暂停帧时，关闭 nic 端口或者在几秒钟内从 rdma 切换到\ntcp。然而这些机制不能处理基于暂停帧的其他问题（deadlock、head-of-line blocking）。后来升级到 lossy\nrdma，禁用暂停帧，避免这些问题。\n\n流量优化。降低流量放大率：EC 和数据压缩。 使用 EC 替代三副本的技术挑战：在 EC 中存储小文件是很昂贵的，因为在具有固定长度的数据上执行 EC\n需要大量的 zero padding。引入了机制避免：小写请求聚合，EC 和三副本动态切换。使用 Intel ISA-L 避免 EC\n计算延迟。网络流量放大率可以从 6.375 倍减少到 4.875 倍。 压缩 FlatLogFile：client 和 GCworker\n写文件前都会进行压缩。使用 LZ4 算法，平均压缩率达到 50%。因此，流量放大率可以从 4.875 倍进一步降低到 2.9375 倍\n\n前台流量和后台流量的动态带宽分配。例如，如果整个存储集群有足够的空闲空间，我们会在时间上降低阈值，限制后台流量（如 GC 流量）的带宽，而让前台流量使用更\n多的带宽。对于淘宝网，盘古在白天到午夜设置了一个低阈值，以应对大量的前端访问请求。午夜过后，盘古增加了门槛，因为前台的流量减少了。\n\n\n内存瓶颈#\n\n盘古的基本内存瓶颈在于网络进程（即执行 DMA 操作的网卡）和接收主机中的应用进程（如数据复制、数据复制和垃圾回收）之间对内存带宽的高度争夺。 由于 NIC\n无法获得足够的内存带宽，因此对 NIC 产生了严重的 PCIe 背压。因此，NIC 的缓冲区被飞行中的数据包填满。最终，它丢弃了溢出的数据包，触发了网络中的拥\n堵控制机制，导致整体性能下降（例如，每台服务器的网络吞吐量下降 30% ，延迟增加 5%-10% ， IOPS\n下降10%）。这种现象并不是盘古独有的。谷歌最近也报道 了这个问题。 三步走解决这个问题：\n\n * 充分利用内存通道\n * 后台流量从 TCP 切换到 RDMA\n * 远程直接缓存访问（RDCA），发送方直接访问接收方的缓存。\n\n小容量 DRAM。我们在服务器上增加了更多小容量的 DRAM（例如 16 GB），充分利用内存通道。启用 NUMA，避免跨 socket\n内存访问[[学习/Ultra path interconnect]]。 后台流量从 TCP 切换到 RDMA。TCP 比 RDMA 至少多 4 次\nmemcpy。切换后，后台流量的内存带宽消耗降低了 75%。为了保证前台流量的 QoS，设计了一个类似 Linux tc 的机制控制背景流量。\n\n远程直接缓存访问。发送方绕过接收方的内存，直接访问其缓存。\n在盘古的生产工作负载中，我们观察到一个重要的现象：数据离开网卡后在内存中停留的时间非常短（即平均数百微秒）。假设网卡离开后的平均时间为 200\nus，对于一个双端口的 100 Gbps 网卡，我们只需要 5 MB 的时间来存储离开网卡的数据 三个组件：\n\n * 缓存驻留的缓冲池。该池使用共享接收队列（SRQ）来接收小消息，并使用配备有基于窗口的速率控制机制的 READ 缓冲区来接收大消息，这样，RDMA\n   操作所需的内存缓冲区就可以放入缓存。\n * 迅速的缓存回收。为了支持 100-Gbps NIC 以尽可能少的 LLC 运行，我们设计了 swift\n   缓存回收机制，通过（1）沿管道并行处理数据，以及（2）使用硬件卸载和轻量级（非）序列化优化处理，减少数据在 NIC 后的时间跨度。\n * 缓存压力意识的逃避机制。为了处理偶尔的抖动（例如、SSD 慢速写入和应用异常），逃逸机制监控保留的 LLC\n   的使用情况，并采取相应的行动，包括（1）通过向驻缓存的缓冲池添加新的缓冲区来替换滞留数据的缓存缓冲区，这样 RDCA\n   中能够容纳新到请求的可用缓存的大小保持不变；（2）如果发生太多的替换，主动将运行缓慢的应用的数据复制到内存，这样其他应用可以使用 RDCA\n   的缓冲池，并且该池不会占用太多的缓存；和 (3) 让 NIC 在拥塞通知数据包中标记显式拥塞通知\n   (ECN)，以便在复制到内存失败或不足以释放缓存压力时表示拥塞。 利用 Intel DDIO 实现了 RDCA。 对于典型的存储工作负载，RDCA\n   在每台服务器上消耗了 12 MB 的 LLC 缓存（占总缓存的 20%），降低了平均内存带宽消耗量减少了 89%，网络吞吐量提高了 9%。\n   我们发现，RDCA 在非存储工作负载中也很有效，例如，它减少了集体的平均延迟。在对延迟敏感的 HPC 应用中，通信速度提高了，达到35.1%。RDCA 于\n   2022 年底在盘古中推出。\n\n\nCPU 瓶颈#\n\n即使通过优化来打破网络和内存瓶颈，盘古在 100 Gbps 网络中的吞吐量仍然只能达到其理论值的 80% 。这是因为数据序列化和反序列化、数据压缩和数据 CRC\n计算等操作会消耗很多 CPU 资源，使 CPU 成为盘古的另一个瓶颈。\n\n混合 RPC。protobuf 对 RPC 请求进行序列化反序列化，花费大约 30% CPU，主要发生在少数 RPC 类型的数据路径上。数据面用\nflatbuffer 原始结构直接发送接收，不进行序列化。控制面继续用 protobuf。每个 CPU 核心的网络吞吐量，增加了约 59%\n\n使用 CPU wait 支持超线程。最初没有开启超线程，随着 CPU 核心的稀缺，开始采用。超线程两个性能问题：\n\n * 同一个物理核心上的超线程需要切换上下文。\n * 一个 ht 执行任务影响另一个 ht，造成两个任务的延迟增加。比如一个 network idle-polling thread 在一个 ht\n   上执行，另一个 ht 跑 lz4 压缩，与压缩线程完全占据物理核心的情况相比，数据压缩的延迟增加 25%。 引入 CPU wait 指令，由\n   monitor 和 mwait 组成，需要少于 5 ms 进行调用。引入 CPU 等待后，network idle-polling thread\n   会在被监控的内存地址处等待，直到该内存地址被其他线程写入时才会被唤醒。在 mwait 过程中，它所运行的 HT 进入空闲睡眠状态，这是 C0 以外的 C\n   状态之一，不会干扰其他 HT。此外，用系统调用唤醒 HT 的时间是 ms 级的。与没有 wait 相比，网络吞吐量增加了31.6%。\n\n> monitor 和 mwait 是一对\n> x86指令集架构中的指令，它们主要用于支持处理器的超线程技术。超线程技术允许一个物理核心模拟两个或更多的逻辑处理器，从而提高处理器的资源利用率，使其能够在同\n> 一时间处理更多的线程。\n> \n> monitor 和 mwait\n> 指令的作用是在某种程度上提高处理器的能效比，它们允许处理器在等待某个特定内存地址的值发生改变时进入低功耗状态。这可以在一些多线程编程场景中节省能源，例如同步\n> 原语（如自旋锁）和其他线程间通信机制。\n> \n> monitor 指令用于设置一个监视区域，即一个特定的内存地址范围。当处理器执行 monitor\n> 指令时，它会监视指定的内存地址，以检测该地址的值是否发生变化。\n> \n> mwait指令用于让处理器进入一种低功耗状态，称为“睡眠”状态。在这种状态下，处理器会暂停执行，等待指定内存地址的值发生变化。当监视的内存地址发生变化时，处\n> 理器会被唤醒并继续执行。\n\nCRC 和压缩卸载到 FPGA。压缩卸载到 FPGA 上，3 GB/s 吞吐节约 10c。CRC 计算卸载到网卡上。CPU 对网卡计算的 CRC\n进行汇总和检查。节约 30% CPU 开销。\n\n\n运营经验#\n\n细粒度的监控和智能诊断。在盘古 2.0 中，我们通过两个关键设计改进了盘古的监控系统，以满足高性能的要求（即 100μs 级的 I/O 延迟）。\n首先，我们将监控的时间粒度从 15 秒提高到 1 秒，并扩展了日志服务[68]来设计一个按需追踪的系统。与盘古 1.0\n中的粗粒度监控相比，这使得我们可以在每个文件操作的基础上进行追踪，以准确捕捉细粒度的异常事件（例如，内存分配异常和日志打印超时）。\n第二，我们采用人工智能来更好地捕捉异常事件和其根源之间的因果关系。推断出的根本原因由运营团队评定，并反馈给训练有素的模型以提高其准确性。这种设计大幅提高了诊断的\n准确性，并减少了所需的人工努力。\n\n端到端 CRC。懒得说了。拜占庭错误。\n\nUSSOS SLA 抖动。比如 tcmalloc 进入 global memory allocation 时（比如一个新线程需要内存）或者 memory\norganization 阶段（too many RDMA queue pairs try to reserve high-order memory\nspace），就会变得过于耗时。引入了一个 userspace 内存分配池，优化了 rdma\n驱动使用匿名页。周期性的重量级任务（比如打印日志）转移到异步线程，避免影响数据操作 SLA。第三，我们发现当内存占用率很高并且 USSOS\n需要执行内存回收时，USSOS 的 CPU 利用率明显增加。因此，我们调整了内存重新收集的阈值，以减少 USSOS\n进入内存重新收集的机会。我们还在后台重新收集分配给缓冲区和高速缓存的内存。\n\n处理 MCE（correctable machine check\nexceptions）提高可用性。最初，USSOS（§3.2）可以监控这种硬件故障，但它不能感知内核如何迁移物理内存以隔离异常。因此，当 USSOS\n试图根据其过时的虚拟-物理内存地址映射来访问已经迁移的物理内存时，就会发生错误。为此，我们在 USSOS 的 MCE\n监控守护程序中加入了一个处理程序。一旦发现可纠正的 MCE\n的数量超过阈值，与之相关的用户空间进程将暂停，让处理程序通知内核迁移内存。迁移之后，进程恢复并在访问内存页之前更新其映射表。这种设计提高了盘古的可用性，而性能的\n下降可以忽略不计。例如，我们在一个 2300 台服务器的集群中观察到 22 天内 < 330 个可纠正的 MCEs。\n\n来自不同供应商的异质性内存带宽。盘古部署了来自不同供应商的内存。我们的测试表明，在1:1 的内存读写比下，三个不同供应商的 128 GB 内存的可实现带宽分别为\n94 GB/s 、 84 GB/s 和 60 GB/s （ 即57%的差异）。这种异质的内存带宽会导致集群的\n性能下降。这一观察告诉我们要更加关注内存的性能，而不是容量。这也是我们最早发现的接收器主机数据通路拥堵的证据，也是 RDCA（§4.2.3）的直接动机。\n\n\n经验教训#\n\n首先，用户空间系统的开发和操作比较简单。例如，我们的数据显示，在内核空间文件系统中，修复错误需要两个月的时间，但在 USSFS\n中只需要几个星期。在用户空间开发新的功能（例如，zoned\nnamespace）需要更少的开发人员和更短的时间。此外，监测和跟踪用户空间系统的行为并相应地调整其参数也比较容易。\n\n第二，开发用户空间系统应该从内核空间的设计中学习。特别是，为了建立一个高性能的 USSOS，我们不仅需要统一存储和网络堆栈，还需要为内存管理、CPU\n调度和硬件故障处理设计用户空间模块。内核空间在这些功能方面相当出色。因此，用户空间系统可以通过向它学习而受益。\n\nPmem。PMem 有很多优点，比如快速的数据持久化，对 RDMA 友好，低读取延迟（PMem 上 6µs 对 SSD 上\n80µs），低尾部延迟，以及对缓存友好。因此，我们在盘古开发了一个基于 PMem 的 30 微秒 EBS 服务。然而，英特尔决定关闭其 PMem\n业务，迫使我们重新考虑这项服务。在开发新的服务时，我们需要更全面的计划\n（例如，考虑替代性、可持续性和成本权衡）。但我们乐观地认为，新的存储类存储器将出现，对这些问题有更好的解决方案。\n\n整个硬件卸载压缩的开发需要一个 20 人的团队花费两年时间，期间我们解决了很多问题，如 FPGA\n硬件成本，压缩数据的完整性，以及与硬件中其他功能的共存。最后，结果的好处大大超过了这个成本。","routePath":"/notes/paper/pangu","lang":"","toc":[{"text":"简介","id":"简介","depth":2,"charIndex":409},{"text":"盘古 1.0：面向容量","id":"盘古-10面向容量","depth":3,"charIndex":451},{"text":"盘古 2.0","id":"盘古-20","depth":2,"charIndex":566},{"text":"第一阶段","id":"第一阶段","depth":3,"charIndex":988},{"text":"第二阶段","id":"第二阶段","depth":2,"charIndex":1241},{"text":"小结","id":"小结","depth":2,"charIndex":1600},{"text":"盘古的概述","id":"盘古的概述","depth":2,"charIndex":1893},{"text":"设计目标","id":"设计目标","depth":2,"charIndex":2718},{"text":"阶段一","id":"阶段一","depth":2,"charIndex":2809},{"text":"Append-only fs","id":"append-only-fs","depth":3,"charIndex":2816},{"text":"Heavyweight Client","id":"heavyweight-client","depth":3,"charIndex":3299},{"text":"Append-only chunk 管理","id":"append-only-chunk-管理","depth":3,"charIndex":3451},{"text":"元数据操作优化","id":"元数据操作优化","depth":3,"charIndex":4003},{"text":"并行化","id":"并行化","depth":4,"charIndex":4117},{"text":"Chunkserver USSOS","id":"chunkserver-ussos","depth":3,"charIndex":5066},{"text":"内存管理","id":"内存管理","depth":3,"charIndex":5458},{"text":"CPU 调度","id":"cpu-调度","depth":3,"charIndex":5724},{"text":"Append-only USSFS","id":"append-only-ussfs","depth":3,"charIndex":6311},{"text":"高性能高可用","id":"高性能高可用","depth":3,"charIndex":6632},{"text":"Chasing","id":"chasing","depth":4,"charIndex":6641},{"text":"Non-stop write","id":"non-stop-write","depth":4,"charIndex":7316},{"text":"Backup read","id":"backup-read","depth":4,"charIndex":7481},{"text":"黑名单","id":"黑名单","depth":4,"charIndex":7688},{"text":"评估","id":"评估","depth":3,"charIndex":8115},{"text":"阶段二：适应性能导向的商业模式","id":"阶段二适应性能导向的商业模式","depth":2,"charIndex":8123},{"text":"网络瓶颈","id":"网络瓶颈","depth":3,"charIndex":8261},{"text":"内存瓶颈","id":"内存瓶颈","depth":3,"charIndex":8853},{"text":"CPU 瓶颈","id":"cpu-瓶颈","depth":3,"charIndex":10281}],"domain":"","frontmatter":{"title":"FAST23 Pangu"},"version":""},{"id":11,"title":"PolarFS: An Ultra-low Latency and Failure Resilient Distributed File System for Shared Storage Cloud Database","content":"PolarFS: An Ultra-low Latency and Failure Resilient Distributed File System for\nShared Storage Cloud Database#\n\nPlease visit","routePath":"/notes/paper/polarfs","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":12,"title":"Zab","content":"#\n\n\nzk 是否能够保证线性一致性？#\n\n很多人可能会觉得既然 zk 支持线性一致性写，那么也可以通过 sync + read 来支持线性一致性读，理论上这样是可以支持线性一致性读的，但在 zk\n真正的实现中是不能严格满足线性一致性的，具体可以参照 jepsen 中的讨论。不能严格满足线性一致性的根据原因就是 zk 在实现过程中并没有将 sync\n当做一个空写日志去执行，而是直接让 leader 返回一个 zxid 给 follower，然而此时的 leader 并没有像 raft 那样通过 read\nindex 发起一轮心跳或 lease read 的方式来确保自己一定是 leader，从而可能在网络分区脑裂的 corner case\n下返回旧数据，因此无法在严格意义上满足线性一致性。当然，这种 corner case 在实际中很少见，而且也应该可以修复，所以从技术上来讲，zk 应该是可以用\nsync + read 来支持线性一致性读的。 Zookeeper 论文阅读\n\n\nZooKeeper vs etcd#\n\n前面的 博客 已经介绍过了，zab 保证的是顺序一致性语义，raft\n保证的则是线性一致性语义。尽管他们都可以算强一致性，但顺序一致性并无时间维度的约束，所以可能并不满足现实世界的时序。也就是说，在现实世界中，顺序一致性是可能返回\n旧数据的。对于一个分布式协调服务，可能返回旧数据实际上是比较坑爹的一件事，尽管 zk 保证了单客户端 FIFO\n的顺序，但有些场景还是有一些受限的。因此在这一点上，我认为 etcd 保证的线性一致性是更好的，zk 的顺序一致性有时候会有坑，这一点 PingCAP 的\nCTO 也在知乎的”分布式之美”圆桌会谈上吐槽过。","routePath":"/notes/zookeeper/zab","lang":"","toc":[{"text":"zk 是否能够保证线性一致性？","id":"zk-是否能够保证线性一致性","depth":2,"charIndex":3},{"text":"ZooKeeper vs etcd","id":"zookeeper-vs-etcd","depth":2,"charIndex":444}],"domain":"","frontmatter":{},"version":""},{"id":13,"title":"ZooKeeper FLE","content":"#\n\nZooKeeper messaging doesn't care about the exact method of electing a leader has\nlong as the following holds:\n\n * The leader has seen the highest zxid of all the followers.\n * A quorum of servers have committed to following the leader.\n\nOf these two requirements only the first, the highest zxid among the followers\nneeds to hold for correct operation.\n\nThe second requirement, a quorum of followers, just needs to hold with high\nprobability. We are going to recheck the second requirement, so if a failure\nhappens during or after the leader election and quorum is lost, we will recover\nby abandoning leader activation and running another election.\n\nAfter leader election a single server will be designated as a leader and start\nwaiting for followers to connect. The rest of the servers will try to connect to\nthe leader. The leader will sync up with followers by sending any proposals they\nare missing, or if a follower is missing too many proposals, it will send a full\nsnapshot of the state to the follower.\n\nReconfig 时会重启 FLE。\n\n\n\nFLE 内部有两个队列：\n\n\n\nsendqueue 对应 WorkerSender 线程。revcqueue 对应 WorkerReceiver 线程。\n\nWorkerReceiver 做的事情：\n\n * Response buffer 解析为 Notification\n * 当收到没有投票权的节点（observer 或者 non-voting follower）的通知时，直接返回一个自己的当前选票\n * 如果当前节点是 LOOKING 状态，则将解析好的 Notification 放进 recvqueue\n   * 如果收到的 Notification 是 LOOKING，并且 epoch 比自己低，就告诉对方自己的 epoch\n * 如果当前节点不是 LOOKING 状态，就告诉对方自己认为的 Leader 是什么\n\nWorkerSender 做的事情：\n\n * 根据 ToSend 中 sid 不停发送\n\n\nlogicalclock#\n\n\n\n初始值为 0\n\n\nlookForLeader#\n\n进入 LOOKING 状态时，会进行 lookForLeader\n\n\n\n此时开始计时：\n\n\n\n一开始，给自己投一票，并且发送一轮通知。\n\n\n\n\n\n开始循环，直到 FLE 停止或者退出 LOOKING 状态\n\n\n\n * 当 n 为 null\n   * 如果发送队列都为空，则再发一轮 Notification\n   * 否则，说明有连接断开了，则 connectAll()\n   * 然后倍增 notTimeout（最大值为 maxNotificationInterval=60s）\n * 当投票者或者投票者投的 leader 不是有效节点（其他集群的节点）时，就忽略\n * 当投票者或者投票者投的 leader 是有效节点时：\n   \n   * n.electionEpoch > logicalclock.get()\n     \n     更新自己的 logicalclock 为 n.electionEpoch，清空 recvset\n     \n     如果满足 totalOrderPredicate，则更新自己的选票为 n，否则投给自己\n   \n   * n.electionEpoch < logicalclock.get()\n     \n     忽略这个 Notification n\n   \n   * n.electionEpoch == logicalclock.get()\n     \n     如果满足 totalOrderPredicate，则更新自己的选票为 n\n\nrecvset 放入 n\n\n如果已经形成共识（投给同个节点的票数过半）则：\n\n * 反复从 recvqueue poll，n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)。\n   * 如果还能 poll 到，说明有节点改变了想法。\n   * 如果 n 满足 totalOrderPredicate，则放入 recvqueue，并退出循环，在下一个大循环中处理 n\n * 如果已经无法从 recvqueue 中 poll 出 notifaction，则说明没有节点改变想法\n   * 根据 n 设置自身状态为 LEADING 或 FOLLOWING\n   * 退出 lookForLeader，返回这个 n 作为 endVote\n\n如果当前状态是 LEADING/FOLLOWING，则 lookForLeader 为：\n\n向所有人发送给自己的 notiy\n\n遍历 recvqueue，加入 epoch 相同的到 recvset。满足 Quorum 后，则返回 endVote\n\n如果遍历 recvqueue 时，epoch 跟其他人返回的不同，则加入到 outofelection\n\n\ntotalOrderPredicate#\n\n返回 true 当下面的任意 case 满足：\n\n 1. New epoch 更高\n 2. New epoch 跟 current epoch 相等，但是 new zxid 更高\n 3. New epoch 与 current epoch 相等，并且 new zxid 与 current zxid 相等，但是 sid 更高\n\n如何判断 notify 来自 LOOKING 时的 lookForLeader？","routePath":"/notes/zookeeper/zookeeper-fle","lang":"","toc":[{"text":"logicalclock","id":"logicalclock","depth":2,"charIndex":1448},{"text":"lookForLeader","id":"lookforleader","depth":2,"charIndex":1474},{"text":"totalOrderPredicate","id":"totalorderpredicate","depth":2,"charIndex":2661}],"domain":"","frontmatter":{},"version":""}]